{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project - Toxic Comment Detection\n",
    "\n",
    "The code below is used for the detection of toxic comments in a given dataset -https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge-. The decision was made to use a RandomForrest model to predict whether a comment is, primarily, toxic; but is also trained to derive whether a comment is: Severely toxic, obscene, a threat, an insult and/or identity hate. \n",
    "\n",
    "The code begins with importing all the relevant libraries, where the RandomForestClassifier from the sklearn library is the main player. After the importation of all the libraries, a function is defined called: cleanText. The function takes the text which has to be cleaned as its input, where after it will start to clean the text as follows:\n",
    "- First  the stopword are imported from the default nltk.corpus where the 'english' stopwords are selected while we deal with -mostly- an English text;\n",
    "- The decision was made to whitelist the following words: 'not','you','your','you're' and are. This while not is a negation and will make a toxic comment untoxic by negating the toxidity and the variations of you while this indicates a direction against someone which is usually used when swearing;\n",
    "- The text is transformed to only be lowercase so we do not have to deal with capital letters, this was also found to lack improvement in our accuracy;\n",
    "- English abbreviations are also subbed in order to get the full spelling;\n",
    "- the stopwords are deleted from the text -of course without the whitelisted words-;\n",
    "- all the numerals are deleted from the data, this while you can hardly be toxic by using numbers;\n",
    "- punctuation is deleted;\n",
    "- in the final step the word tokens are joined together to get back to the original data form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "\n",
    "import collections\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from string import punctuation\n",
    "import math\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "#cleaning the available text to get clean data\n",
    "def cleanText(text):\n",
    "\n",
    "    stopw = stopwords.words('english')\n",
    "\n",
    "    # remove 'not' from the stopwords while this can negate an insult\n",
    "    # decided to add 'you' while in toxic conversations you is used to enhance te meaning\n",
    "    #TODO: enhance comments\n",
    "    stopw.remove('not')\n",
    "    stopw.remove('you')\n",
    "    stopw.remove('your')\n",
    "    stopw.remove('you\\'re')\n",
    "    stopw.remove('are')\n",
    "\n",
    "    #make the whole text lowercase so we don't make differences between capitalization\n",
    "    text = text.lower()\n",
    "\n",
    "    #subbing words to match cleaner words.\n",
    "    text = re.sub(\"\\'s\", \" \", text)\n",
    "    text = re.sub(\" whats \", \" what is \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'n't \", \" not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\" n't \", \" not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"I'm\", \"I am\", text)\n",
    "    text = re.sub(\"shouldn\\'t\", \" should not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"were\\'nt\", \" were not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"can't\", \" can not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(\"\\'ll\", \" will \", text)\n",
    "\n",
    "    #remove all the stopwords and remove all non letters\n",
    "    words = word_tokenize(text)\n",
    "    tokens = [word for word in words if word not in stopw]\n",
    "\n",
    "    #remove all non letters in the dataset\n",
    "    tokens = [word for word in tokens if re.match(r'[^\\W\\d]*$', word)]\n",
    "\n",
    "    #remove all URLs in the dataset\n",
    "    # TODO: write a regex for this\n",
    "\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    #remove punctuation\n",
    "    text = ''.join([word for word in text if word not in punctuation])\n",
    "\n",
    "    #dealing with empty data line\n",
    "    if type(text) != str or text == '':\n",
    "        return ''\n",
    "\n",
    "\n",
    "    cleaned_text = text\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having finished a method to clean the dataset, it is imported and the method is applied to the dataFrame. This way the cleaned text is now inserted into the dataFrame which is easier to work with later on. The commented code was a way to investigate the text and what was needed to be cleaned, this is done in an easier way outside of an IDE and into dataset software like Excel. The original text was also exported to get a view on how well the original text was being cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the training data using pandas.\n",
    "df_train = pd.read_csv('Data/train.csv')\n",
    "\n",
    "# save the original text to easily inspect it and derive what has to be cleaned\n",
    "# df_train['comment_text'].to_csv('Data/OriginalText.csv')\n",
    "\n",
    "# save the cleaned text to easily inspect it\n",
    "# df_train['comment_text'].to_csv('Data/cleanedText.csv')\n",
    "\n",
    "# clean the text\n",
    "df_train['comment_text'] = df_train['comment_text'].apply(cleanText)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataFrame for the RandomForestClassifier\n",
    "\n",
    "With having a clean dataset the first step is completed towards implementing the randomForestClassifier. However, more preparation has to be done. The problem stands that the RandomForestClassifier -from here on to be abbreviated as RFC- needs an input of floats and the current dataFrame only holds strings as an input. Thus, in a way, these strings have to be converted to floats. In this case, the choice was made to do this by using a word2vec model from the Gensim library. Word2Vec will give a vector value to every word in the dataset, in order to make this useful for the dataset the average value of all vectors in a sentence is taken such that we have one value representing every sentence. In order to do this, two methods were constructed:\n",
    "\n",
    "averageVecValue: \n",
    "   - Inputs: \n",
    "        comment; the sentence which has to be averaged\n",
    "        model; the word2vec model\n",
    "        vectorSize; the vectorSize that was chosen to initialize the word2vec model\n",
    "        vocab; the vocabulary made by the word2vec model\n",
    "\n",
    "the method takes every word in the comment and checks if it is in the vocabulary, these words are all added and finally divided by the vectorSize to compute an average value. \n",
    "\n",
    "Word2Vec:    \n",
    "    - Inputs: \n",
    "        cleanedData; is the cleaned dataSet, to be more specific the cleaned comments\n",
    "        dataSet; the full dataFrame\n",
    "  \n",
    "initializes the word2vec model from the gensim library and uses the averageVecValue to compute the average vector value of a sentence, all of the comment's respective vector values are saved in the list vectorizedData where after it is passed back.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averageVecValue(comment, model, vectorSize, vocab):\n",
    "    Vector = np.zeros(vectorSize)\n",
    "    \n",
    "    for word in comment:\n",
    "        if word in vocab:\n",
    "            Vector += np.array(model.wv.get_vector(word))\n",
    "    \n",
    "    Vector_value = np.divide(Vector, vectorSize)\n",
    "    \n",
    "    return Vector_value.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def word2Vec(cleanedData, dataSet):\n",
    "    dataSet['comment_text_tokenized'] = dataSet['comment_text'].apply(word_tokenize)\n",
    "    tokens = df_train['comment_text_tokenized']\n",
    "    \n",
    "    vectorSize = 300\n",
    "    word2vec = Word2Vec(tokens,min_count = 2, size = vectorSize)\n",
    "    vocab = word2vec.wv.vocab\n",
    "    \n",
    "    vectorizedData = []\n",
    "    for index, row in dataSet.iterrows():\n",
    "        vectorizedData.append(averageVecValue(row['comment_text'], word2vec, vectorSize, vocab))\n",
    "    \n",
    "    return vectorizedData\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing the RFC\n",
    "\n",
    "To initialize the RFC, the dataSet is first reduced to a sample size of 25000. This while computation would be to long for testing and the goal of the project is not to waste a lot of precious time by watching tests running. the Xtrain variable is the computed word2vec vectors for every comment and the Ytrain are the labels already given with our dataSet. Because we are running for the first time and it is interesting to see how the model will perform without perfect parameters; the RFC has its basic parameters. \n",
    "\n",
    "The training data is split into a train and test set and the training data is fitted to the RFC. To get a simple view of its accuracy the .score method is used; however this is not a really good indication. To get a good indication, the choice was made to let the RFC make a prediction and use Metrics from the sklearn library to get a better view of the model's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up training data \n",
      "started training the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Martijn\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy: 89.08%\n"
     ]
    }
   ],
   "source": [
    "# setting up the X training comments (vectorize them to be able to be used as input for model) and Y training labels\n",
    "print(\"setting up training data \")\n",
    "df_train = df_train.sample(n=25000, random_state=33)\n",
    "Xtrain = word2Vec(df_train['comment_text'], df_train)\n",
    "Ytrain = df_train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "\n",
    "\n",
    "# importing the test data set to test the algorithm\n",
    "df_test = pd.read_csv('Data/test.csv')\n",
    "\n",
    "\n",
    "#run the algorithm for the first time and get an idea of the accuracy with the basic parameters.\n",
    "print(\"started training the model\")\n",
    "rf_model = RandomForestClassifier()\n",
    "# rf_model.fit(Xtrain, Ytrain)\n",
    "\n",
    "# test the accuracy of the model on a split training dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "xtrain, xtest,ytrain,ytest = train_test_split(Xtrain,Ytrain,test_size=0.33, random_state=66)\n",
    "rf_model.fit(xtrain, ytrain)\n",
    "print(\"RF Accuracy: %0.2f%%\" % (100 * rf_model.score(xtest, ytest)))\n",
    "\n",
    "#predictions\n",
    "rf_predict = rf_model.predict(xtest)\n",
    "rfc_cv_score = cross_val_score(rf_model, Xtrain, Ytrain, cv=10, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests\n",
    "\n",
    "The tests conducted can be found below. From these we can see that the model performs quite well with just basic parameters. To analyse the score of the program, we primarily look at the micro averages. This, while it takes into account that the size of the classes is unbalanced. The micro average precision gives us the number 0.57, which means that if the algorithm puts a comment in one of the classes, it is 0.57 percent certain that is correct.\n",
    "However, the recall sits at a very low score of 0.14. This indicates that the model fails to classify a class 86 percent of the time while it is applicable, which is not optimal for obvious reasons.\n",
    "The AUC score is calculated with a 10 cross validation calculation. This score lies at 0.67, which shows that the algorithm does well in distinguishing toxic and non-toxic comments. \n",
    "\n",
    "However this is just with the basic parameters of the RFC. To get a higher accuracy we can try to predict which hyperparameter values would be applicable. Therefore, the choice was made to implement a RandomizedSearchCV, which will be explained later.    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[8210    1    0    0    0]\n",
      " [  24    0    0    0    0]\n",
      " [   0    0    0    1    0]\n",
      " [  11    0    0    0    0]\n",
      " [   3    0    0    0    0]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.14      0.22       798\n",
      "           1       0.46      0.07      0.13        81\n",
      "           2       0.61      0.15      0.24       418\n",
      "           3       0.00      0.00      0.00        24\n",
      "           4       0.59      0.12      0.20       409\n",
      "           5       1.00      0.01      0.02        88\n",
      "\n",
      "   micro avg       0.57      0.12      0.20      1818\n",
      "   macro avg       0.53      0.08      0.13      1818\n",
      "weighted avg       0.58      0.12      0.20      1818\n",
      " samples avg       0.01      0.01      0.01      1818\n",
      "\n",
      "\n",
      "\n",
      "All Cross Validation Scores\n",
      "[0.63975405 0.67487891 0.68695161 0.67402996 0.68495634 0.64221961\n",
      " 0.66450689 0.68059341 0.64031439 0.70533164]\n",
      "\n",
      "\n",
      "Mean Cross Validation Score\n",
      "Mean AUC Score - Random Forest:  0.6693536806830096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Martijn\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Martijn\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Martijn\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#printing tests \n",
    "#TODO: explain the results... \n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(ytest.values.argmax(axis=1), rf_predict.argmax(axis=1)))\n",
    "print('\\n')\n",
    "\n",
    "# TODO: recall and f-score warning should be fixed \n",
    "print(\"Classification Report\")\n",
    "print(classification_report(ytest, rf_predict))\n",
    "print('\\n')\n",
    "\n",
    "print(\"All Cross Validation Scores\")\n",
    "print(rfc_cv_score)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "print(\"Mean Cross Validation Score\")\n",
    "print(rfc_cv_score_optimized.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test of the optimized model and RandomizedSearchCV \n",
    "\n",
    "As already said, a RandomizedSearchCV was used to get the best hyperParameters for the RFC. What a RandomizedSearchCV basically does is giving random values to your randomGrid parameters and testing the model for a -predefined- number of models. \n",
    "\n",
    "To elaborate, first the parameters that will be investigated have to be chosen. In this case, these are chosen to be the hyperparameters -read most important parameters- of the RFC; being: the number of estimators, the maximum amount of features, the maximum tree depth and the bootstrap Boolean value. \n",
    "The iterations of the RandomSearchCV is set to 50 and these 50 models will be checked twice. Therefore, in total the amount of iterations the algorithm will make is 100; this again chosen due to not wanting to spend hours waiting for the algorithm.  After the algorithm is finished the best parameters found can be retrieved and inserted in the new model which will then again be tested. The found parameters in my initial run where:  n_estimators = 200 , max_features = 'sqrt' , max_depth = 140 , bootstrap = 'true'. \n",
    "\n",
    "Finally, to test if the model was improved by the found hyper parameters the same tests are conducted as before and this resulted in improvements on multiple scores. \n",
    "First of all, the precision was increased to 0.67 compared to the previous 0.57. This indicates a higher certainty of correctness when the classifier does say a class to be applicable to a training example. The AUC score was also improved upon, which we hypothesize to be so because of the higher precision. Furthermore, this means that the optimized model is even better at distinguishing toxic and non-toxic comments.  \n",
    "However, after the optimization the recall stayed at the same score of 0.14, which still is not desirable. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning\n",
      "Optimized Model Accuracy\n",
      "RF Accuracy: 89.59%\n",
      "predicting\n",
      "Optimized model Confusion Matrix:\n",
      "[[8211    0    0    0    0]\n",
      " [  24    0    0    0    0]\n",
      " [   1    0    0    0    0]\n",
      " [  11    0    0    0    0]\n",
      " [   3    0    0    0    0]]\n",
      "\n",
      "\n",
      "Optimized Model Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.13      0.21       798\n",
      "           1       0.58      0.09      0.15        81\n",
      "           2       0.69      0.16      0.26       418\n",
      "           3       0.00      0.00      0.00        24\n",
      "           4       0.69      0.12      0.21       409\n",
      "           5       1.00      0.01      0.02        88\n",
      "\n",
      "   micro avg       0.67      0.12      0.21      1818\n",
      "   macro avg       0.60      0.08      0.14      1818\n",
      "weighted avg       0.67      0.12      0.21      1818\n",
      " samples avg       0.01      0.01      0.01      1818\n",
      "\n",
      "\n",
      "\n",
      "Optimized Model All Cross Validation Scores\n",
      "[0.75822184 0.77522289 0.78791895 0.78586201 0.77958223 0.7506177\n",
      " 0.80440595 0.7838306  0.73707963 0.79953228]\n",
      "\n",
      "\n",
      "Optimized Model Mean Cross Validation Score\n",
      "0.7762274088140708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Martijn\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Martijn\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# testing if the retrieved parameters are actually better\n",
    "# parameters;\n",
    "# n_estimators = 200 , max_features = 'sqrt' , max_depth = 140 , bootstrap = 'true'\n",
    "    \n",
    "#initializing the model and fitting it to the training data\n",
    "print(\"beginning\")\n",
    "\n",
    "\n",
    "rf_optimized_model = RandomForestClassifier(n_estimators=200, max_features='sqrt', max_depth=140)\n",
    "rf_optimized_model.fit(xtrain,ytrain)\n",
    "\n",
    "# normal score of the model\n",
    "print(\"Optimized Model Accuracy\")\n",
    "print(\"RF Accuracy: %0.2f%%\" % (100 * rf_optimized_model.score(xtest, ytest)))\n",
    "\n",
    "\n",
    "print(\"predicting\")\n",
    "# prediction and crossvalidation\n",
    "rf_predict_optimized = rf_optimized_model.predict(xtest)\n",
    "rfc_cv_score_optimized = cross_val_score(rf_optimized_model, Xtrain, Ytrain, cv=10, scoring='roc_auc')\n",
    "\n",
    "# tests\n",
    "# TODO: explain the results.\n",
    "print(\"Optimized model Confusion Matrix:\")\n",
    "print(confusion_matrix(ytest.values.argmax(axis=1), rf_predict_optimized.argmax(axis=1)))\n",
    "print('\\n')\n",
    "\n",
    "#f1 score and recall things should be fixed\n",
    "print(\"Optimized Model Classification Report\")\n",
    "print(classification_report(ytest, rf_predict_optimized))\n",
    "print('\\n')\n",
    "\n",
    "print(\"Optimized Model All Cross Validation Scores\")\n",
    "print(rfc_cv_score_optimized)\n",
    "print('\\n')\n",
    "\n",
    "print(\"Optimized Model Mean Cross Validation Score\")\n",
    "print(rfc_cv_score_optimized.mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improve the model by trying to find the best parameters for the random forest, we can do this by using RandomizedSearchedCV\n",
    "# hyperparameter training, using 4 parameters\n",
    "# only has to be run once!\n",
    "# be careful for overfitting\n",
    "# https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 with modified parameters\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n",
    "\n",
    "# number of features at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# max depth\n",
    "max_depth = [int(x) for x in np.linspace(100, 500, num=11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# bootstrap\n",
    "bootstrap = ['true', 'false']\n",
    "\n",
    "# create random grid\n",
    "random_grid = {\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_features': max_features,\n",
    "        'max_depth': max_depth,\n",
    "        'bootstrap': bootstrap\n",
    "}\n",
    "\n",
    "# Random search of parameters\n",
    "rfc_random = RandomizedSearchCV(estimator=rf_model, param_distributions=random_grid, n_iter=50, cv=2, verbose=2,\n",
    "                                    random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "rfc_random.fit(xtrain, ytrain)\n",
    "# print the best result\n",
    "print(rfc_random.best_params_)\n",
    "\n",
    "#result = {'n_estimators': 200, 'max_features': 'sqrt', 'max_depth': 140, 'bootstrap': 'true'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial run of RandomizedSearchedCV\n",
    "\n",
    "Best parameter print:  {'n_estimators': 200, 'max_features': 'sqrt', 'max_depth': 140, 'bootstrap': 'true'}\n",
    "\n",
    "\n",
    "# Output:\n",
    "\n",
    "# Fitting 2 folds for each of 50 candidates, totalling 100 fits\n",
    "\n",
    "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
    "[CV] n_estimators=800, max_features=sqrt, max_depth=220, bootstrap=true \n",
    "[CV] n_estimators=800, max_features=sqrt, max_depth=220, bootstrap=true \n",
    "[CV] n_estimators=1000, max_features=sqrt, max_depth=420, bootstrap=false \n",
    "[CV] n_estimators=1000, max_features=sqrt, max_depth=420, bootstrap=false \n",
    "[CV] n_estimators=1000, max_features=sqrt, max_depth=380, bootstrap=false \n",
    "[CV] n_estimators=1000, max_features=sqrt, max_depth=380, bootstrap=false \n",
    "[CV] n_estimators=1600, max_features=sqrt, max_depth=140, bootstrap=false \n",
    "[CV] n_estimators=1600, max_features=sqrt, max_depth=140, bootstrap=false \n",
    "8[CV]  n_estimators=800, max_features=sqrt, max_depth=220, bootstrap=true, total= 4.3min\n",
    "[CV] n_estimators=2000, max_features=sqrt, max_depth=380, bootstrap=false \n",
    "[CV]  n_estimators=800, max_features=sqrt, max_depth=220, bootstrap=true, total= 4.7min\n",
    "[CV] n_estimators=2000, max_features=sqrt, max_depth=380, bootstrap=false \n",
    "[CV]  n_estimators=1000, max_features=sqrt, max_depth=420, bootstrap=false, total= 5.8min\n",
    "[CV] n_estimators=600, max_features=sqrt, max_depth=420, bootstrap=true \n",
    "[CV]  n_estimators=1000, max_features=sqrt, max_depth=380, bootstrap=false, total= 5.8min\n",
    "[CV] n_estimators=600, max_features=sqrt, max_depth=420, bootstrap=true \n",
    "[CV]  n_estimators=1000, max_features=sqrt, max_depth=420, bootstrap=false, total= 6.2min\n",
    "[CV] n_estimators=1800, max_features=auto, max_depth=140, bootstrap=false \n",
    "[CV]  n_estimators=1000, max_features=sqrt, max_depth=380, bootstrap=false, total= 6.1min\n",
    "[CV] n_estimators=1800, max_features=auto, max_depth=140, bootstrap=false \n",
    "21[CV]  n_estimators=600, max_features=sqrt, max_depth=420, bootstrap=true, total= 3.6min\n",
    "[CV] n_estimators=1200, max_features=sqrt, max_depth=340, bootstrap=false \n",
    "[CV]  n_estimators=600, max_features=sqrt, max_depth=420, bootstrap=true, total= 3.7min\n",
    "[CV] n_estimators=1200, max_features=sqrt, max_depth=340, bootstrap=false \n",
    "[CV]  n_estimators=1600, max_features=sqrt, max_depth=140, bootstrap=false, total= 9.5min\n",
    "[CV] n_estimators=1600, max_features=sqrt, max_depth=220, bootstrap=true \n",
    "[CV]  n_estimators=1600, max_features=sqrt, max_depth=140, bootstrap=false, total=10.0min\n",
    "[CV] n_estimators=1600, max_features=sqrt, max_depth=220, bootstrap=true \n",
    "29[CV]  n_estimators=2000, max_features=sqrt, max_depth=380, bootstrap=false, total=12.3min\n",
    "[CV] n_estimators=600, max_features=auto, max_depth=500, bootstrap=false \n",
    "[CV]  n_estimators=1200, max_features=sqrt, max_depth=340, bootstrap=false, total= 7.3min\n",
    "[CV] n_estimators=600, max_features=auto, max_depth=500, bootstrap=false \n",
    "[CV]  n_estimators=1200, max_features=sqrt, max_depth=340, bootstrap=false, total= 7.5min\n",
    "[CV] n_estimators=2000, max_features=auto, max_depth=100, bootstrap=true \n",
    "[CV]  n_estimators=1800, max_features=auto, max_depth=140, bootstrap=false, total=11.2min\n",
    "[CV] n_estimators=2000, max_features=auto, max_depth=100, bootstrap=true \n",
    "[CV]  n_estimators=2000, max_features=sqrt, max_depth=380, bootstrap=false, total=13.2min\n",
    "[CV] n_estimators=400, max_features=auto, max_depth=180, bootstrap=false \n",
    "[CV]  n_estimators=1800, max_features=auto, max_depth=140, bootstrap=false, total=11.4min\n",
    "[CV] n_estimators=400, max_features=auto, max_depth=180, bootstrap=false \n",
    "[CV]  n_estimators=1600, max_features=sqrt, max_depth=220, bootstrap=true, total=10.0min\n",
    "[CV] n_estimators=800, max_features=sqrt, max_depth=460, bootstrap=false \n",
    "[CV]  n_estimators=400, max_features=auto, max_depth=180, bootstrap=false, total= 2.7min\n",
    "[CV] n_estimators=800, max_features=sqrt, max_depth=460, bootstrap=false \n",
    "[CV]  n_estimators=600, max_features=auto, max_depth=500, bootstrap=false, total= 4.2min\n",
    "[CV] n_estimators=1800, max_features=sqrt, max_depth=180, bootstrap=false \n",
    "[CV]  n_estimators=400, max_features=auto, max_depth=180, bootstrap=false, total= 2.7min\n",
    "[CV] n_estimators=1800, max_features=sqrt, max_depth=180, bootstrap=false \n",
    "[CV]  n_estimators=600, max_features=auto, max_depth=500, bootstrap=false, total= 4.3min\n",
    "[CV]  n_estimators=1600, max_features=sqrt, max_depth=220, bootstrap=true, total=10.5min\n",
    "[CV] n_estimators=200, max_features=sqrt, max_depth=140, bootstrap=true \n",
    "[CV] n_estimators=200, max_features=sqrt, max_depth=140, bootstrap=true \n",
    "[CV]  n_estimators=200, max_features=sqrt, max_depth=140, bootstrap=true, total= 1.2min\n",
    "[CV] n_estimators=800, max_features=sqrt, max_depth=260, bootstrap=true \n",
    "[CV]  n_estimators=200, max_features=sqrt, max_depth=140, bootstrap=true, total= 1.4min\n",
    "[CV] n_estimators=800, max_features=sqrt, max_depth=260, bootstrap=true \n",
    "[CV]  n_estimators=800, max_features=sqrt, max_depth=460, bootstrap=false, total= 5.2min\n",
    "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed: 26.5min\n",
    "[CV] n_estimators=1200, max_features=sqrt, max_depth=180, bootstrap=true \n",
    "[CV]  n_estimators=800, max_features=sqrt, max_depth=460, bootstrap=false, total= 5.5min\n",
    "[CV] n_estimators=1200, max_features=sqrt, max_depth=180, bootstrap=true \n",
    "[CV]  n_estimators=800, max_features=sqrt, max_depth=260, bootstrap=true, total= 5.3min\n",
    "[CV] n_estimators=800, max_features=auto, max_depth=460, bootstrap=false \n",
    "[CV]  n_estimators=800, max_features=sqrt, max_depth=260, bootstrap=true, total= 5.2min\n",
    "[CV] n_estimators=800, max_features=auto, max_depth=460, bootstrap=false \n",
    "[CV]  n_estimators=2000, max_features=auto, max_depth=100, bootstrap=true, total=13.1min\n",
    "[CV] n_estimators=800, max_features=sqrt, max_depth=140, bootstrap=true \n",
    "[CV]  n_estimators=2000, max_features=auto, max_depth=100, bootstrap=true, total=14.0min\n",
    "[CV] n_estimators=800, max_features=sqrt, max_depth=140, bootstrap=true \n",
    "[CV]  n_estimators=1200, max_features=sqrt, max_depth=180, bootstrap=true, total= 7.6min\n",
    "[CV] n_estimators=800, max_features=auto, max_depth=500, bootstrap=true \n",
    "[CV]  n_estimators=1800, max_features=sqrt, max_depth=180, bootstrap=false, total=12.1min\n",
    "[CV]  n_estimators=800, max_features=auto, max_depth=460, bootstrap=false, total= 5.4min\n",
    "[CV] n_estimators=800, max_features=auto, max_depth=500, bootstrap=true \n",
    "[CV]  n_estimators=1800, max_features=sqrt, max_depth=180, bootstrap=false, total=12.3min\n",
    "[CV]  n_estimators=800, max_features=auto, max_depth=460, bootstrap=false, total= 5.3min\n",
    "[CV] n_estimators=200, max_features=auto, max_depth=460, bootstrap=true \n",
    "[CV] n_estimators=200, max_features=auto, max_depth=460, bootstrap=true \n",
    "[CV] n_estimators=1200, max_features=auto, max_depth=None, bootstrap=false \n",
    "[CV]  n_estimators=1200, max_features=sqrt, max_depth=180, bootstrap=true, total= 8.1min\n",
    "[CV] n_estimators=1200, max_features=auto, max_depth=None, bootstrap=false \n",
    "[CV]  n_estimators=200, max_features=auto, max_depth=460, bootstrap=true, total= 1.3min\n",
    "[CV] n_estimators=200, max_features=sqrt, max_depth=220, bootstrap=true \n",
    "[CV]  n_estimators=200, max_features=auto, max_depth=460, bootstrap=true, total= 1.4min\n",
    "[CV] n_estimators=200, max_features=sqrt, max_depth=220, bootstrap=true \n",
    "[CV]  n_estimators=800, max_features=sqrt, max_depth=140, bootstrap=true, total= 5.0min\n",
    "[CV] n_estimators=400, max_features=auto, max_depth=140, bootstrap=false \n",
    "[CV]  n_estimators=200, max_features=sqrt, max_depth=220, bootstrap=true, total= 1.4min\n",
    "[CV] n_estimators=400, max_features=auto, max_depth=140, bootstrap=false \n",
    "[CV]  n_estimators=200, max_features=sqrt, max_depth=220, bootstrap=true, total= 1.3min\n",
    "[CV] n_estimators=1000, max_features=auto, max_depth=300, bootstrap=true \n",
    "[CV]  n_estimators=800, max_features=sqrt, max_depth=140, bootstrap=true, total= 5.6min\n",
    "[CV] n_estimators=1000, max_features=auto, max_depth=300, bootstrap=true \n",
    "[CV]  n_estimators=400, max_features=auto, max_depth=140, bootstrap=false, total= 2.4min\n",
    "[CV] n_estimators=1000, max_features=sqrt, max_depth=None, bootstrap=false \n",
    "[CV]  n_estimators=800, max_features=auto, max_depth=500, bootstrap=true, total= 5.0min\n",
    "[CV] n_estimators=1000, max_features=sqrt, max_depth=None, bootstrap=false \n",
    "[CV]  n_estimators=800, max_features=auto, max_depth=500, bootstrap=true, total= 5.7min\n",
    "[CV] n_estimators=1800, max_features=sqrt, max_depth=300, bootstrap=false \n",
    "[CV]  n_estimators=400, max_features=auto, max_depth=140, bootstrap=false, total= 2.8min\n",
    "[CV] n_estimators=1800, max_features=sqrt, max_depth=300, bootstrap=false \n",
    "[CV]  n_estimators=1200, max_features=auto, max_depth=None, bootstrap=false, total= 7.6min\n",
    "[CV] n_estimators=400, max_features=auto, max_depth=260, bootstrap=false \n",
    "[CV]  n_estimators=1200, max_features=auto, max_depth=None, bootstrap=false, total= 8.3min\n",
    "[CV] n_estimators=400, max_features=auto, max_depth=260, bootstrap=false \n",
    "[CV]  n_estimators=1000, max_features=auto, max_depth=300, bootstrap=true, total= 6.3min\n",
    "[CV] n_estimators=400, max_features=auto, max_depth=220, bootstrap=false \n",
    "[CV]  n_estimators=400, max_features=auto, max_depth=260, bootstrap=false, total= 2.6min\n",
    "[CV] n_estimators=400, max_features=auto, max_depth=220, bootstrap=false \n",
    "[CV]  n_estimators=1000, max_features=auto, max_depth=300, bootstrap=true, total= 6.9min\n",
    "[CV] n_estimators=200, max_features=auto, max_depth=100, bootstrap=true \n",
    "[CV]  n_estimators=1000, max_features=sqrt, max_depth=None, bootstrap=false, total= 6.4min\n",
    "[CV] n_estimators=200, max_features=auto, max_depth=100, bootstrap=true \n",
    "[CV]  n_estimators=1000, max_features=sqrt, max_depth=None, bootstrap=false, total= 6.5min\n",
    "[CV] n_estimators=400, max_features=sqrt, max_depth=380, bootstrap=false \n",
    "[CV]  n_estimators=400, max_features=auto, max_depth=260, bootstrap=false, total= 2.7min\n",
    "[CV] n_estimators=400, max_features=sqrt, max_depth=380, bootstrap=false \n",
    "[CV]  n_estimators=400, max_features=auto, max_depth=220, bootstrap=false, total= 2.5min\n",
    "[CV]  n_estimators=200, max_features=auto, max_depth=100, bootstrap=true, total= 1.2min\n",
    "[CV] n_estimators=400, max_features=sqrt, max_depth=100, bootstrap=true \n",
    "[CV]  n_estimators=200, max_features=auto, max_depth=100, bootstrap=true, total= 1.3min\n",
    "[CV] n_estimators=400, max_features=sqrt, max_depth=100, bootstrap=true \n",
    "[CV] n_estimators=200, max_features=auto, max_depth=380, bootstrap=false \n",
    "[CV]  n_estimators=400, max_features=auto, max_depth=220, bootstrap=false, total= 2.6min\n",
    "[CV] n_estimators=200, max_features=auto, max_depth=380, bootstrap=false \n",
    "[CV]  n_estimators=400, max_features=sqrt, max_depth=380, bootstrap=false, total= 2.4min\n",
    "[CV]  n_estimators=200, max_features=auto, max_depth=380, bootstrap=false, total= 1.2min\n",
    "[CV] n_estimators=200, max_features=sqrt, max_depth=460, bootstrap=false \n",
    "[CV] n_estimators=200, max_features=sqrt, max_depth=460, bootstrap=false \n",
    "[CV]  n_estimators=200, max_features=auto, max_depth=380, bootstrap=false, total= 1.3min\n",
    "[CV] n_estimators=1600, max_features=sqrt, max_depth=340, bootstrap=false \n",
    "[CV]  n_estimators=400, max_features=sqrt, max_depth=380, bootstrap=false, total= 2.7min\n",
    "[CV]  n_estimators=400, max_features=sqrt, max_depth=100, bootstrap=true, total= 2.5min\n",
    "[CV] n_estimators=1600, max_features=sqrt, max_depth=340, bootstrap=false \n",
    "[CV] n_estimators=1600, max_features=sqrt, max_depth=300, bootstrap=true \n",
    "[CV]  n_estimators=400, max_features=sqrt, max_depth=100, bootstrap=true, total= 2.6min\n",
    "[CV]  n_estimators=200, max_features=sqrt, max_depth=460, bootstrap=false, total= 1.3min\n",
    "[CV] n_estimators=1600, max_features=sqrt, max_depth=300, bootstrap=true \n",
    "[CV]  n_estimators=200, max_features=sqrt, max_depth=460, bootstrap=false, total= 1.3min\n",
    "[CV] n_estimators=600, max_features=auto, max_depth=220, bootstrap=false \n",
    "[CV] n_estimators=600, max_features=auto, max_depth=220, bootstrap=false \n",
    "[CV]  n_estimators=1800, max_features=sqrt, max_depth=300, bootstrap=false, total=11.8min\n",
    "[CV] n_estimators=800, max_features=sqrt, max_depth=420, bootstrap=true \n",
    "[CV]  n_estimators=1800, max_features=sqrt, max_depth=300, bootstrap=false, total=12.3min\n",
    "[CV] n_estimators=800, max_features=sqrt, max_depth=420, bootstrap=true \n",
    "[CV]  n_estimators=600, max_features=auto, max_depth=220, bootstrap=false, total= 3.7min\n",
    "[CV] n_estimators=400, max_features=sqrt, max_depth=None, bootstrap=false \n",
    "[CV]  n_estimators=600, max_features=auto, max_depth=220, bootstrap=false, total= 3.9min\n",
    "[CV] n_estimators=400, max_features=sqrt, max_depth=None, bootstrap=false \n",
    "[CV]  n_estimators=400, max_features=sqrt, max_depth=None, bootstrap=false, total= 2.4min\n",
    "[CV] n_estimators=1800, max_features=sqrt, max_depth=100, bootstrap=true \n",
    "[CV]  n_estimators=400, max_features=sqrt, max_depth=None, bootstrap=false, total= 2.5min\n",
    "[CV] n_estimators=1800, max_features=sqrt, max_depth=100, bootstrap=true \n",
    "[CV]  n_estimators=800, max_features=sqrt, max_depth=420, bootstrap=true, total= 5.3min\n",
    "[CV] n_estimators=600, max_features=sqrt, max_depth=260, bootstrap=false \n",
    "[CV]  n_estimators=800, max_features=sqrt, max_depth=420, bootstrap=true, total= 5.3min\n",
    "[CV] n_estimators=600, max_features=sqrt, max_depth=260, bootstrap=false \n",
    "[CV]  n_estimators=1600, max_features=sqrt, max_depth=340, bootstrap=false, total= 9.9min\n",
    "[CV] n_estimators=1200, max_features=auto, max_depth=460, bootstrap=true \n",
    "[CV]  n_estimators=1600, max_features=sqrt, max_depth=300, bootstrap=true, total=10.0min\n",
    "[CV] n_estimators=1200, max_features=auto, max_depth=460, bootstrap=true \n",
    "[CV]  n_estimators=1600, max_features=sqrt, max_depth=340, bootstrap=false, total=10.9min\n",
    "[CV] n_estimators=200, max_features=sqrt, max_depth=260, bootstrap=true \n",
    "[CV]  n_estimators=1600, max_features=sqrt, max_depth=300, bootstrap=true, total=11.0min\n",
    "[CV] n_estimators=200, max_features=sqrt, max_depth=260, bootstrap=true \n",
    "[CV]  n_estimators=600, max_features=sqrt, max_depth=260, bootstrap=false, total= 4.1min\n",
    "[CV] n_estimators=200, max_features=auto, max_depth=None, bootstrap=true \n",
    "[CV]  n_estimators=200, max_features=sqrt, max_depth=260, bootstrap=true, total= 1.4min\n",
    "[CV] n_estimators=200, max_features=auto, max_depth=None, bootstrap=true \n",
    "[CV]  n_estimators=200, max_features=sqrt, max_depth=260, bootstrap=true, total= 1.5min\n",
    "[CV] n_estimators=2000, max_features=sqrt, max_depth=140, bootstrap=true \n",
    "[CV]  n_estimators=600, max_features=sqrt, max_depth=260, bootstrap=false, total= 4.3min\n",
    "[CV] n_estimators=2000, max_features=sqrt, max_depth=140, bootstrap=true \n",
    "[CV]  n_estimators=200, max_features=auto, max_depth=None, bootstrap=true, total= 1.4min\n",
    "[CV] n_estimators=800, max_features=auto, max_depth=260, bootstrap=false \n",
    "[CV]  n_estimators=200, max_features=auto, max_depth=None, bootstrap=true, total= 1.6min\n",
    "[CV] n_estimators=800, max_features=auto, max_depth=260, bootstrap=false \n",
    "[CV]  n_estimators=1200, max_features=auto, max_depth=460, bootstrap=true, total= 7.5min\n",
    "[CV] n_estimators=2000, max_features=sqrt, max_depth=500, bootstrap=false \n",
    "[CV]  n_estimators=1200, max_features=auto, max_depth=460, bootstrap=true, total= 7.8min\n",
    "[CV] n_estimators=2000, max_features=sqrt, max_depth=500, bootstrap=false \n",
    "[CV]  n_estimators=1800, max_features=sqrt, max_depth=100, bootstrap=true, total=11.4min\n",
    "[CV] n_estimators=1400, max_features=sqrt, max_depth=220, bootstrap=true \n",
    "[CV]  n_estimators=800, max_features=auto, max_depth=260, bootstrap=false, total= 5.2min\n",
    "[CV] n_estimators=1400, max_features=sqrt, max_depth=220, bootstrap=true \n",
    "[CV]  n_estimators=1800, max_features=sqrt, max_depth=100, bootstrap=true, total=11.9min\n",
    "[CV]  n_estimators=800, max_features=auto, max_depth=260, bootstrap=false, total= 5.3min\n",
    "[CV] n_estimators=1200, max_features=sqrt, max_depth=380, bootstrap=true \n",
    "[CV] n_estimators=1200, max_features=sqrt, max_depth=380, bootstrap=true \n",
    "[CV]  n_estimators=2000, max_features=sqrt, max_depth=140, bootstrap=true, total=12.5min\n",
    "[CV] n_estimators=800, max_features=auto, max_depth=220, bootstrap=true \n",
    "[CV]  n_estimators=1200, max_features=sqrt, max_depth=380, bootstrap=true, total= 7.1min\n",
    "[CV] n_estimators=800, max_features=auto, max_depth=220, bootstrap=true \n",
    "[CV]  n_estimators=2000, max_features=sqrt, max_depth=140, bootstrap=true, total=13.0min\n",
    "[CV]  n_estimators=1400, max_features=sqrt, max_depth=220, bootstrap=true, total= 8.3min\n",
    "[CV]  n_estimators=1200, max_features=sqrt, max_depth=380, bootstrap=true, total= 7.4min\n",
    "[CV]  n_estimators=1400, max_features=sqrt, max_depth=220, bootstrap=true, total= 8.7min\n",
    "[CV]  n_estimators=2000, max_features=sqrt, max_depth=500, bootstrap=false, total=11.3min\n",
    "[CV]  n_estimators=800, max_features=auto, max_depth=220, bootstrap=true, total= 3.5min\n",
    "[CV]  n_estimators=2000, max_features=sqrt, max_depth=500, bootstrap=false, total=11.3min\n",
    "[CV]  n_estimators=800, max_features=auto, max_depth=220, bootstrap=true, total= 3.3min\n",
    "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 80.2min finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
